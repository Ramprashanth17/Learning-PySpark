{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c1c1af-2f1f-45f1-b308-20a704c0a603",
   "metadata": {},
   "source": [
    "### Spark Session\n",
    "\n",
    "Entry point to PySpark's functionality within a program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8103cb-7203-452c-9ebb-66bb2114eb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/08 00:43:55 WARN Utils: Your hostname, OnePiece, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/01/08 00:43:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/08 00:43:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "### Building SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession   ### SparkSession entry point located in pyspark.sql pkg, providing functionality for data transformation\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder     ### Builder pattern abstraction for constructing a sparksession, where we chain the methods to configure the entry point.\n",
    "         .appName(\"Analyzing the vocabulary of Pride and Prejudice.\") ### Relevant appName helping in identifying which programs run on the Spark cluster\n",
    "         .getOrCreate()) ### Program works in both interactive or batch mode by avoiding creating a new session if one already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02f8a14-0338-4854-8df9-f9a3f4423e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' SparkSession is the wrapper around sparkContext which uses dataframe as it is more versatile and fast as the main datastructure over lower-level RDD'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext ### For using low-level RDD\n",
    "\n",
    "\"\"\" SparkSession is the wrapper around sparkContext which uses dataframe as it is more versatile and fast as the main datastructure over lower-level RDD\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6435ea02-db03-4079-be7d-899b00af71fc",
   "metadata": {},
   "source": [
    "### Setting the Log Level\n",
    "\n",
    "Monitoring PySpark jobs is an important part of developing. PySpark has many levels of logging, from nothing to full description of everything happening in cluster. pyspark shell defaults on WARN, which is a bit chatty.  And, PySpark program defaults to INFO level which is oversharing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b23948-ebd8-4153-8f00-b3e9368a258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Changing the log level to keyword\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"KEYWORD\")\n",
    "\n",
    "### Keywords: WARN, ALL, INFO, DEBUG, TRACE, OFF, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db94e78-c20f-459c-991d-aea460e18f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_df',\n",
       " '_jreader',\n",
       " '_set_opts',\n",
       " '_spark',\n",
       " 'csv',\n",
       " 'format',\n",
       " 'jdbc',\n",
       " 'json',\n",
       " 'load',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orc',\n",
       " 'parquet',\n",
       " 'schema',\n",
       " 'table',\n",
       " 'text',\n",
       " 'xml']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71081450-baad-4ea4-8942-f3aa71fb9112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
