{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c1c1af-2f1f-45f1-b308-20a704c0a603",
   "metadata": {},
   "source": [
    "### Spark Session\n",
    "\n",
    "Entry point to PySpark's functionality within a program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8103cb-7203-452c-9ebb-66bb2114eb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/09 18:01:28 WARN Utils: Your hostname, OnePiece, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/01/09 18:01:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/09 18:01:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "### Building SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession   ### SparkSession entry point located in pyspark.sql pkg, providing functionality for data transformation\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder     ### Builder pattern abstraction for constructing a sparksession, where we chain the methods to configure the entry point.\n",
    "         .appName(\"Analyzing the vocabulary of Pride and Prejudice.\") ### Relevant appName helping in identifying which programs run on the Spark cluster\n",
    "         .getOrCreate()) ### Program works in both interactive or batch mode by avoiding creating a new session if one already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02f8a14-0338-4854-8df9-f9a3f4423e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' SparkSession is the wrapper around sparkContext which uses dataframe as it is more versatile and fast as the main datastructure over lower-level RDD'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext ### For using low-level RDD\n",
    "\n",
    "\"\"\" SparkSession is the wrapper around sparkContext which uses dataframe as it is more versatile and fast as the main datastructure over lower-level RDD\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6435ea02-db03-4079-be7d-899b00af71fc",
   "metadata": {},
   "source": [
    "### Setting the Log Level\n",
    "\n",
    "Monitoring PySpark jobs is an important part of developing. PySpark has many levels of logging, from nothing to full description of everything happening in cluster. pyspark shell defaults on WARN, which is a bit chatty.  And, PySpark program defaults to INFO level which is oversharing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b23948-ebd8-4153-8f00-b3e9368a258c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Supplied level KEYWORD did not match one of: ALL,DEBUG,ERROR,WARN,INFO,OFF,FATAL,TRACE",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m### Changing the log level to keyword\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetLogLevel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mKEYWORD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m### Keywords: WARN, ALL, INFO, DEBUG, TRACE, OFF, etc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-workspace/venv/lib/python3.12/site-packages/pyspark/core/context.py:562\u001b[39m, in \u001b[36mSparkContext.setLogLevel\u001b[39m\u001b[34m(self, logLevel)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msetLogLevel\u001b[39m(\u001b[38;5;28mself\u001b[39m, logLevel: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    547\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[33;03m    Control our logLevel. This overrides any user-defined log settings.\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[33;03m    Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    560\u001b[39m \u001b[33;03m    >>> sc.setLogLevel(\"WARN\")  # doctest :+SKIP\u001b[39;00m\n\u001b[32m    561\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetLogLevel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogLevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-workspace/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-workspace/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: requirement failed: Supplied level KEYWORD did not match one of: ALL,DEBUG,ERROR,WARN,INFO,OFF,FATAL,TRACE"
     ]
    }
   ],
   "source": [
    "### Changing the log level to keyword\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"KEYWORD\")\n",
    "\n",
    "### Keywords: WARN, ALL, INFO, DEBUG, TRACE, OFF, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db94e78-c20f-459c-991d-aea460e18f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71081450-baad-4ea4-8942-f3aa71fb9112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Reading the csv file\n",
    "\n",
    "book = spark.read.text(\"./gutenberg.txt\")\n",
    "\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c35ac7e-115e-4f81-ad3e-dd54431abb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To get the schema of the dataframe\n",
    "\n",
    "book.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31edc540-f51e-4aaf-93f1-6d914f40cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For documentation use\n",
    "print(spark.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ddb8f-ca1c-472a-abab-8583d3132a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the contents of the dataframe\n",
    "\n",
    "book.show(n=10, truncate = False, vertical = True) # Shows 20 rows and truncates long values.\n",
    "## n = no.of rows, truncate = default truncates at 20 chars, vertical = displays each record as a small table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211acfa-b5a3-4299-87bf-83f518085f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generally spark is lazily evaluated, but you want eager evaluation, similar to pandas you can change the mode\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                     .config(\"spark.sql.repl.eagerEval.enabled\", \"True\")\n",
    "                     .getOrCreate())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38d4f344-fb9f-461f-aad5-31efa6e91758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[almost, no, rest...|\n",
      "|[re-use, it, unde...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "### Step-2 Tokenize the words \n",
    "\n",
    "#### We'll be splitting the lines of text into arrays or words\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "lines = book.select(split(book.value,\" \").alias(\"line\")) ## Each record is stored in value. Splitting with space as separator\n",
    "\n",
    "lines. show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2f0cd-95e6-4de5-8d86-99a00edf6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Otherways to select a value column from the dataframe\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "book.select(book.value)\n",
    "book.select(book[\"value\"]) # Use this when the col names contains any special chars\n",
    "book.select(col[\"value\"]) # No need to mention the dataframe\n",
    "book.select(\"value\") # Might become problematic in case of transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28c835e-ef9c-4959-8343-97c60ee5fc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- split(value,  , -1): array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Transforming columns: Splitting a string into a list of words\n",
    "\n",
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "lines = book.select(split(col(\"value\"), \" \"))\n",
    "\n",
    "lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a910e0-4168-4f9a-bd12-88a8780b20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.show(5) ## [] represents array, if its empty it means there are no values present or that row is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db4b172d-32fc-46ad-82a5-933c26613e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- line: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Aliasing \n",
    "book.select(split(col(\"value\"), \" \").alias(\"line\")). printSchema() ###You can use .withColumnRenamed method as well to alias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a20499-7c80-4854-b13e-4d2e55dc7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = book.select(split(book.value, \" \"))\n",
    "lines = lines.withColumnRenamed(\"split(value, , -1)\", \"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70546e09-6efc-4054-9d99-98f8c65ddc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = book.select(split(book.value, \" \").alias(\"line\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bb3aca4-8fbf-4d5f-b950-a7f16e675875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|Prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "|    Austen|\n",
      "|          |\n",
      "|      This|\n",
      "|     eBook|\n",
      "|        is|\n",
      "+----------+\n",
      "only showing top 15 rows\n"
     ]
    }
   ],
   "source": [
    "#### Reshaping the data: Exploding a list into Rows\n",
    "\n",
    "\"\"\"After splitting the records, we have arrays of strings and it would be better to have one record for each word\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "\n",
    "words.show(15)\n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad792c8a-a949-4012-9fdc-6f5847bed3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|     pride|\n",
      "|       and|\n",
      "|prejudice,|\n",
      "|        by|\n",
      "|      jane|\n",
      "|    austen|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "### Step-3 Removing punctuation and turning into lower case\n",
    "\n",
    "from pyspark.sql.functions import lower\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "\n",
    "words_lower.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "046cdfaa-0466-4f6e-ba13-1627eaa61dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|         |\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word_lower\"), \"[a-z]+\" , 0).alias(\"word\")) # We only match for multiple lowercase chars, + will match for one or more occurences\n",
    "\n",
    "words_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eb97edc-229e-4271-94ff-16099d117b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "### Filtering Rows\n",
    "\n",
    "words_nonnull = words_clean.filter(col(\"word\") != \"\")\n",
    "words_nonnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "362f5cd3-439e-4c06-a72a-d5e9c8f9c144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupedData[grouping expressions: [word], value: [word: string], type: GroupBy]\n"
     ]
    }
   ],
   "source": [
    "### Counting the word frequencies\n",
    "\n",
    "groups = words_nonnull.groupby(col(\"word\"))\n",
    "\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "422f1ae6-a475-4c70-959e-5bac63f67412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         word|count|\n",
      "+-------------+-----+\n",
      "|       online|    4|\n",
      "|         some|  209|\n",
      "|        still|   72|\n",
      "|          few|   72|\n",
      "|         hope|  122|\n",
      "|        those|   60|\n",
      "|     cautious|    4|\n",
      "|    imitation|    1|\n",
      "|          art|    3|\n",
      "|      solaced|    1|\n",
      "|       poetry|    2|\n",
      "|    arguments|    5|\n",
      "| premeditated|    1|\n",
      "|      elevate|    1|\n",
      "|       doubts|    2|\n",
      "|    destitute|    1|\n",
      "|    solemnity|    5|\n",
      "|   lieutenant|    1|\n",
      "|gratification|    1|\n",
      "|    connected|   14|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "results = words_nonnull.groupby(col(\"word\")).count()\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "443baf33-2ebf-4697-aa33-3074076bca7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|    12|  812|\n",
      "|     1| 4116|\n",
      "|    13|  393|\n",
      "|     6| 9276|\n",
      "|    16|    5|\n",
      "|     3|28831|\n",
      "|     5|11998|\n",
      "|    15|   32|\n",
      "|     9| 5165|\n",
      "|    17|    3|\n",
      "|     4|22213|\n",
      "|     8| 5121|\n",
      "|     7| 8679|\n",
      "|    10| 2455|\n",
      "|    11| 1386|\n",
      "|    14|  107|\n",
      "|     2|23856|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, length\n",
    "\n",
    "no_of_words_per_lc = words_nonnull.select(length(col(\"word\")).alias(\"length\")).groupby(\"length\").count()\n",
    "\n",
    "no_of_words_per_lc.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82aab0f0-e414-463d-a96f-2e480a6e0c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4496|\n",
      "|  to| 4235|\n",
      "|  of| 3719|\n",
      "| and| 3602|\n",
      "| her| 2223|\n",
      "|   i| 2052|\n",
      "|   a| 1997|\n",
      "|  in| 1920|\n",
      "| was| 1844|\n",
      "| she| 1703|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "### Ordering the results on screen\n",
    "\n",
    "results.orderBy(\"count\", ascending = False).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "988f4119-fb25-4bee-aa4b-55d0fc6f257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4496|\n",
      "|  to| 4235|\n",
      "|  of| 3719|\n",
      "| and| 3602|\n",
      "| her| 2223|\n",
      "|   i| 2052|\n",
      "|   a| 1997|\n",
      "|  in| 1920|\n",
      "| was| 1844|\n",
      "| she| 1703|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "### Or, you can order them in this way\n",
    "\n",
    "results.orderBy(col(\"count\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c60984e-82f8-49cd-96ed-731efe1e806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Writing the df to a file\n",
    "\n",
    "results.write.csv(\"./simple_count.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566546e-22a3-477c-b2c6-96097668d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To manipulate the partitions use coalesce\n",
    "\n",
    "results.coalesce(1).write.csv(\"/count_csv.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
