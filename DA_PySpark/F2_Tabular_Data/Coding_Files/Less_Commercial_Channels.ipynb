{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102d32fd-f433-4c02-8db4-6c647239d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ordering Channels by decreasing proportion of commercials\n",
    "\n",
    "\n",
    "## Step-1: Creating the SparkSession and changing the loglevel\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Getting the Canadian TV channels with the highest/lowest proportion of commercials\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6dd470b-8859-4298-822b-fa79d960b934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/10 17:53:57 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BroadcastLogID: integer (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- LogDate: date (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: timestamp (nullable = true)\n",
      " |-- EndTime: timestamp (nullable = true)\n",
      " |-- LogEntryDate: date (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: timestamp (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      "\n",
      "+--------------+------------+----------+----------+-------------------+----------------------+----------+---------------+-----------------+----------------+---------------+------------------+--------------+--------------------+------------+-------------------+-------------------+------------+------------+--------------------+-------------------+--------+--------------------+------------------+----------------------+-------------+---------+---------+---------+---------+\n",
      "|BroadcastLogID|LogServiceID|   LogDate|SequenceNO|AudienceTargetAgeID|AudienceTargetEthnicID|CategoryID|ClosedCaptionID|CountryOfOriginID|DubDramaCreditID|EthnicProgramID|ProductionSourceID|ProgramClassID|FilmClassificationID|ExhibitionID|           Duration|            EndTime|LogEntryDate|ProductionNO|        ProgramTitle|          StartTime|Subtitle|NetworkAffiliationID|SpecialAttentionID|BroadcastOriginPointID|CompositionID|Producer1|Producer2|Language1|Language2|\n",
      "+--------------+------------+----------+----------+-------------------+----------------------+----------+---------------+-----------------+----------------+---------------+------------------+--------------+--------------------+------------+-------------------+-------------------+------------+------------+--------------------+-------------------+--------+--------------------+------------------+----------------------+-------------+---------+---------+---------+---------+\n",
      "|    1196192316|        3157|2018-08-01|         1|                  4|                  NULL|        13|              3|                3|            NULL|           NULL|                10|            19|                NULL|           2|2026-01-10 02:00:00|2026-01-10 08:00:00|  2018-08-01|      A39082|   Newlywed and Dead|2026-01-10 06:00:00|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|       94|     NULL|\n",
      "|    1196192317|        3157|2018-08-01|         2|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|            20|                NULL|        NULL|2026-01-10 00:00:30|2026-01-10 06:13:45|  2018-08-01|        NULL|15-SPECIALTY CHAN...|2026-01-10 06:13:15|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "|    1196192318|        3157|2018-08-01|         3|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2026-01-10 00:00:15|2026-01-10 06:14:00|  2018-08-01|        NULL|3-PROCTER & GAMBL...|2026-01-10 06:13:45|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "|    1196192319|        3157|2018-08-01|         4|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2026-01-10 00:00:15|2026-01-10 06:14:15|  2018-08-01|        NULL|12-CREDIT KARMA-B...|2026-01-10 06:14:00|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "|    1196192320|        3157|2018-08-01|         5|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2026-01-10 00:00:15|2026-01-10 06:14:30|  2018-08-01|        NULL|3-L'OREAL CANADA-...|2026-01-10 06:14:15|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "+--------------+------------+----------+----------+-------------------+----------------------+----------+---------------+-----------------+----------------+---------------+------------------+--------------+--------------------+------------+-------------------+-------------------+------------+------------+--------------------+-------------------+--------+--------------------+------------------+----------------------+-------------+---------+---------+---------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "### Reading the required data sources\n",
    "import os\n",
    "DIRECTORY = \"../../F2_Tabular_Data/Data/\"\n",
    "\n",
    "logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    "    sep = \"|\",\n",
    "    header = True,\n",
    "    inferSchema = True\n",
    ")\n",
    "\n",
    "logs.printSchema()\n",
    "logs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af113416-b850-47ef-ba85-8f762f426ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LogIdentifierID: string (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- PrimaryFG: integer (nullable = true)\n",
      "\n",
      "+---------------+------------+---------+\n",
      "|LogIdentifierID|LogServiceID|PrimaryFG|\n",
      "+---------------+------------+---------+\n",
      "|           13ST|        3157|        1|\n",
      "|         2000SM|        3466|        1|\n",
      "|           70SM|        3883|        1|\n",
      "|           80SM|        3590|        1|\n",
      "|           90SM|        3470|        1|\n",
      "+---------------+------------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "log_identifier = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables/LogIdentifier.csv\"),\n",
    "    sep = \"|\",\n",
    "    header = True,\n",
    "    inferSchema = True\n",
    ")\n",
    "\n",
    "log_identifier.printSchema()\n",
    "log_identifier.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f4c0e0-1f4e-409d-89c4-80b2d31aa13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_category = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables/CD_Category.csv\"),\n",
    "    sep = \"|\",\n",
    "    header= True,\n",
    "    inferSchema = True\n",
    "). select(\n",
    "    \"CategoryID\",\n",
    "    \"CategoryCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"Category_Description\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eab7dbf-32f1-40f3-88db-6cfc3d38c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_program_class = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables/CD_ProgramClass.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"ProgramClassID\",\n",
    "    \"ProgramClassCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"ProgramClass_Description\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3177c1a4-b94f-44a5-8acd-f0af37e34064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step-3: Data Processing\n",
    "\n",
    "logs = logs.drop(\"BroadcastLogID\", \"SequenceNO\")\n",
    "\n",
    "\n",
    "# Creating a new col \"duration_seconds\"\n",
    "logs = logs.withColumn(\n",
    "    \"duration_seconds\",\n",
    "    (\n",
    "        F.col(\"Duration\").substr(1, 2).cast(\"int\") * 60 * 60\n",
    "        + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\n",
    "        + F.col(\"Duration\").substr(7, 2).cast(\"int\")\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dc35f9d-fdd8-4021-b25f-de6c170b38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_identifier = log_identifier.where(F.col(\"PrimaryFG\") == 1)\n",
    "\n",
    "logs_and_channels = logs.join(log_identifier, \"LogServiceID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a0c5a9c-d61c-4853-bdd9-b31e8c8c4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_log = logs_and_channels.join(cd_category, \"CategoryID\", how=\"left\").join(\n",
    "    cd_program_class, \"ProgramClassID\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67f97928-33cd-4487-9041-c650a79f74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = (\n",
    "    full_log.groupby(\"LogIdentifierID\")\n",
    "    .agg(\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                F.trim(F.col(\"ProgramClassCD\")).isin(\n",
    "                    [\"COM\", \"PRC\", \"PGI\", \"PRO\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]\n",
    "                ),\n",
    "                F.col(\"duration_seconds\"),\n",
    "            ).otherwise(0)\n",
    "        ).alias(\"duration_commercial\"),\n",
    "        F.sum(\"duration_seconds\").alias(\"duration_total\"),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"commercial_ratio\", F.col(\"duration_commercial\") / F.col(\"duration_total\")\n",
    "    )\n",
    "    .fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "747451a2-046e-4eaa-9ee6-c95d6265511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/10 18:03:52 ERROR Executor: Exception in task 3.0 in stage 15.0 (TID 26)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '6-' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 11 in cell [8]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:147)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/10 18:03:52 ERROR Executor: Exception in task 5.0 in stage 15.0 (TID 28)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '6-' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 11 in cell [8]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:147)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/10 18:03:52 ERROR Executor: Exception in task 0.0 in stage 15.0 (TID 23)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '6-' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 11 in cell [8]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:147)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/10 18:03:52 ERROR Executor: Exception in task 2.0 in stage 15.0 (TID 25)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '6-' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 11 in cell [8]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:147)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/10 18:03:52 ERROR Executor: Exception in task 1.0 in stage 15.0 (TID 24)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '6-' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 11 in cell [8]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:147)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/10 18:03:52 WARN TaskSetManager: Lost task 3.0 in stage 15.0 (TID 26) (10.255.255.254 executor driver): org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '6-' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 11 in cell [8]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:147)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "26/01/10 18:03:52 ERROR Executor: Exception in task 4.0 in stage 15.0 (TID 27)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '6-' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 11 in cell [8]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:147)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/10 18:03:52 ERROR Executor: Exception in task 6.0 in stage 15.0 (TID 29)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '6-' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 11 in cell [8]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:147)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/10 18:03:52 ERROR Executor: Exception in task 7.0 in stage 15.0 (TID 30)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '6-' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 11 in cell [8]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:147)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/10 18:03:52 ERROR TaskSetManager: Task 3 in stage 15.0 failed 1 times; aborting job\n",
      "26/01/10 18:03:52 ERROR Executor: Exception in task 8.0 in stage 15.0 (TID 31)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '6-' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 11 in cell [8]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:147)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "{\"ts\": \"2026-01-10 18:03:52.465\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value '6-' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"line 11 in cell [8]\", \"line\": \"\", \"fragment\": \"cast\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o202.showString.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '6-' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"cast\\\" was called from\\nline 11 in cell [8]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:147)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\\n\\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/ram/spark-workspace/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"263\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/ram/spark-workspace/venv/lib/python3.12/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "NumberFormatException",
     "evalue": "[CAST_INVALID_INPUT] The value '6-' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 11 in cell [8]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNumberFormatException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43manswer\u001b[49m\u001b[43m.\u001b[49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcommercial_ratio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-workspace/venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-workspace/venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:316\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    309\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    310\u001b[39m         messageParameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m    313\u001b[39m         },\n\u001b[32m    314\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-workspace/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-workspace/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mNumberFormatException\u001b[39m: [CAST_INVALID_INPUT] The value '6-' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 11 in cell [8]\n"
     ]
    }
   ],
   "source": [
    "answer.orderBy(\"commercial_ratio\", ascending=False).show(1000, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
